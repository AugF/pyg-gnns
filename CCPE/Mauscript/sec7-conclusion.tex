\section{Conclusion}
\label{sec:conclusion}

In this work, we systematically explore the performance bottleneck in graph neural network training. 
We model the existing GNNs with the message-passing framework. 
We classify the GNNs according to their edge and vertex calculation complexities to select four typical GNNs for evaluation. 
The experimental results validate our complexity analysis. Fixing other hyper-parameters, the training time and the memory usage increase linearly with each hyper-parameter of the four GNNs. 
To find out the performance bottleneck in the training time, we decompose the training time per epoch on different levels. 
The training time breakdown analysis indicates that the edge calculation stage and its related basic operators are the performance bottleneck for most GNNs. 
Moreover, the intermediate results produced by the edge calculation stage cause high memory usage, limiting the data scalability. 
Adopting sampling techniques can reduce the training time and the memory usage significantly. 
However, the current implementation of the sampling techniques in PyG brings considerable sampling overheads. 
The small sampled subgraphs cannot make full use of the computing power of a GPU card either. 
Our analysis indicates that the edge calculation stage should be the main target of optimizations. 
Reducing its memory usage and improving its efficiency can significantly improve the performance of the GNN training. 
Based on the analysis, we propose several potential optimizations for the GNN frameworks. 
We believe that our analysis can help developers to have a better understanding of the characteristics of GNN training.

\section*{Acknowledgements}

This work is funded in part by National Key R\&D Program of China [grant number 2019YFC1711000]; China NSF Grants [grant number U1811461]; Jiangsu Province Industry Support Program [grant number BE2017155]; Natural Science Foundation of Jiangsu Province [grant number BK20170651]; Collaborative Innovation Center of Novel Software Technology and Industrialization; and the program B for Outstanding PhD candidate of Nanjing University.
