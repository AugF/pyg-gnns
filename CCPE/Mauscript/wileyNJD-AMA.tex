
\documentclass[AMA,STIX1COL]{WileyNJD-v2}
\usepackage{lineno,hyperref}
\usepackage{amsmath, amsthm}
\usepackage{relsize}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{lscape}
\usepackage{xcolor}
\usepackage{subfig}
\usepackage{float}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\articletype{Article Type}%

\received{}
\revised{}
\accepted{}
        
\raggedbottom

\begin{document}

\title{This is the sample article title}

\author[1]{Zhaokang Wang*}
\author[2,3]{Yunpan Wang, Chunfeng Yuan}
\author[3]{Rong Gu, Yihua Huang}

\authormark{AUTHOR ONE \textsc{et al}}


\address[1]{\orgdiv{Pasa Lab}, \orgname{Nanjing University}, \orgaddress{\state{JiangSu}, \country{China}}}
\address[2]{\orgdiv{Pasa Lab}, \orgname{Nanjing University}, \orgaddress{\state{JiangSu}, \country{China}}}
\address[3]{\orgdiv{Pasa Lab}, \orgname{Nanjing University}, \orgaddress{\state{JiangSu}, \country{China}}}

\corres{*Rong Gu, *Yihua Huang. \email{\{gurong, yhuang\}@nju.edu.cn}}

\presentaddress{No.163, Xianlin Road, Qixia District, Nanjing City, Jiangsu Province, China}

\abstract[Summary]{The graph neural network (GNN) has become a popular research area for its state-of-the-art performance in many graph analysis tasks.
Recently, various graph neural network libraries have emerged.
They make the development of GNNs convenient, but their performance on large datasets is not satisfying.
In this work, we analyze the performance bottleneck in training GNN with GPUs empirically.
A GNN layer can be decomposed into two parts: the vertex and the edge calculation parts.
According to their computational complexity, we select four representative GNNs (GCN, GGNN, GAT, GaAN) for evaluation.
We breakdown their training time and memory usage, evaluate the effects of hyper-parameters, and assess the efficiency of the sampling techniques.  
The experimental evaluation indicates that the edge-related calculation is the performance bottleneck for most GNNs, dominating the training time and memory usage.
Future optimization can focus on it. 
The sampling techniques are essential for training big graphs on GPU, but their current implementations still have room for improvement.}

\keywords{graph neural network, performance bottleneck analysis, empirical evaluation, machine learning system, GPU}

\maketitle

\input{sec1-introduction.tex}
\input{sec2-review-of-graph-neural-network.tex}
\input{sec3-experiment-design.tex}
\input{sec4-experiments.tex}
\input{sec5-insights.tex}
\input{sec6-related-work.tex}
\input{sec7-conclusion.tex}

\nocite{*}% Show all bib entries - both cited and uncited; comment this line to view only cited bib entries;
\bibliography{wileyNJD-AMA}%

\end{document}
