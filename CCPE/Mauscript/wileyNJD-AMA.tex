
\documentclass[AMA,STIX1COL]{WileyNJD-v2}
\usepackage{lineno,hyperref}
\usepackage{amsmath, amsthm}
\usepackage{relsize}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{lscape}
\usepackage{xcolor}
\usepackage{subfig}
\usepackage{float}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\articletype{Article Type}%

\received{}
\revised{}
\accepted{}
        
\raggedbottom

\begin{document}

\title{This is the sample article title}

\author[1]{Zhaokang Wang, Yunpan Wang, Chunfeng Yuan, Rong Gu*, Yihua Huang*}

\authormark{Wang \textsc{et al}}

\address{\orgdiv{State Key Laboratory for Novel Software Technology}, \orgdiv{Department of Computer Science and Technology}, \orgname{Nanjing University}, \orgaddress{\state{Jiangsu}, \country{China}}}

\corres{Rong Gu* and Yihua Huang* with equal contribution. \\ \email{\{gurong, yhuang\}@nju.edu.cn}}

\presentaddress{No.163 Xianlin Avenue,  Nanjing 210023, Jiangsu, China}

\abstract[Summary]{The graph neural network (GNN) has become a popular research area for its state-of-the-art performance in many graph analysis tasks.
Recently, various graph neural network libraries have emerged.
They make the development of GNNs convenient, but their performance on large datasets is not satisfying.
In this work, we analyze the performance bottleneck in training GNN with GPUs empirically.
A GNN layer can be decomposed into two parts: the vertex and the edge calculation parts.
According to their computational complexity, we select four representative GNNs (GCN, GGNN, GAT, GaAN) for evaluation.
We breakdown their training time and memory usage, evaluate the effects of hyper-parameters, and assess the efficiency of the sampling techniques.  
The experimental evaluation indicates that the edge-related calculation is the performance bottleneck for most GNNs, dominating the training time and memory usage.
Future optimization can focus on it. 
The sampling techniques are essential for training big graphs on GPU, but their current implementations still have room for improvement.}

\keywords{graph neural network, performance bottleneck analysis, empirical evaluation, machine learning system, GPU}

\maketitle

\input{sec1-introduction.tex}
\input{sec2-review-of-graph-neural-network.tex}
\input{sec3-experiment-design.tex}
\input{sec4-experiments.tex}
\input{sec5-insights.tex}
\input{sec6-related-work.tex}
\input{sec7-conclusion.tex}

\nocite{*}% Show all bib entries - both cited and uncited; comment this line to view only cited bib entries;
\bibliography{wileyNJD-AMA}%

\end{document}
