\section{Experiment Design}

We design a series of experiments to find out the performance bottleneck in training graph neural networks.
We first introduce the experimental setting in Section~\ref{sec:experimental_env} and then give out our experimental scheme in Section~\ref{sec:experimental_scheme}.
The experiment results are presented and analyzed later in Section~\ref{sec:experiment_results}.

\subsection{Experimental Setting}
\label{sec:experimental_env}

\paragraph{Experimental Environment}
All the experiments were conducted in a CentOS 7 Linux server with the kernel version 3.10.0.
The server had 40 cores and 90 GB main meory.
The server was also equipped with a NVIDIA Tesla T4 GPU card with 16GB GDDR6 memory.
For the software environment, we adopted Python 3.7.7, PyTorch 1.5.0 and CUDA 10.1.
We implemented all the GNNs with PyTorch Geometric 1.5.0.
The execution time decomposition was collected with NVIDIA Nsight systems.

\paragraph{Dataset}
We used six real-world graph datasets as listed in \tablename~\ref{tab:dataset_overview} that were popular in the GNN accuracy evaluation \cite{yang2016_revisiting_semisupervise, zeng2020_graphsaint, shchur2018_pitfall_of_gnn}.
For directed graphs, PyG converts them into undirected ones during the data loading.
Thus, the average degree of a directed graph $\bar{d}=\frac{2|\mathcal{E}|}{|\mathcal{V}|}$.
For an undirected graph, $\mathcal{E}$ already contains two-direction edges and $\bar{d}=\frac{|\mathcal{E}|}{|\mathcal{V}|}$.
For the \texttt{cam} dataset, we generated random dense feature vectors.
Since we mainly focus on the training efficiency of GNNs instead of accuracies, we also used random graphs in the experiments.
To evaluate the effects of graph topological characteristics (like the average degree) on the performance bottleneck uniformly, we used the R-MAT graph generator \cite{rmat-generator}.
Input feature vectors of the random graphs were random dense vectors with the dimension of 32.
Vertices were divided into 10 classes randomly.

\begin{table}
    \centering
    \small
    \begin{tabular}{cccccccc}
        \toprule
        Dataset                                                 & $|\mathcal{V}|$ & $|\mathcal{E}|$ & $\bar{d}$ & $|\boldsymbol{x}|$ & Sparsity & \#Class & Directed \\
        \midrule
        pubmed (pub) \cite{yang2016_revisiting_semisupervised}  & 19,717          & 44,324          & 4.5       & 500                & 0.90     & 3       & Yes      \\
        amazon-photo (amp) \cite{shchur2018_pitfall_of_gnn}     & 7,650           & 119,081         & 31.1      & 745                & 0.65     & 8       & Yes      \\
        amazon-computers (amc) \cite{shchur2018_pitfall_of_gnn} & 13,752          & 245,861         & 35.8      & 767                & 0.65     & 10      & Yes      \\
        coauthor-physics (cph) \cite{shchur2018_pitfall_of_gnn} & 34,493          & 247,962         & 14.4      & 8415               & 0.996    & 5       & Yes      \\
        flickr (fli) \cite{zeng2020_graphsaint}                 & 89,250          & 899,756         & 10.1      & 500                & 0.54     & 7       & No       \\
        com-amazon (cam) \cite{ang2012_defining}                & 334,863         & 925,872         & 2.8       & 32                 & 0.0      & 10      & No       \\
        \bottomrule
    \end{tabular}
    \caption{Dataset overview. $\bar{d}$ represents the average vertex degree. $|\boldsymbol{x}|$ is the dimension of the input feature vector. The sparsity is the proportion of zero elements in the input feature vectors.}
    \label{tab:dataset_overview}
\end{table}

\paragraph{Learning Task}
We used the node classification as the target task in GNNs due to its popularity in real-world applications.
We trained GNNs with the semi-supervised learning setting.
All veritces and their input feature vectors were used used, but only parts of the vertices were attached with labels during the training and they were used to calculate the loss and gradients.
The vertices with unseed labels were used in the evaluation phsae to check the accuracy of the current parameters.
%Since model parameters of GNNs were not restricted by the topological structure of $\mathcal{G}$, the model learned from the semi-supervised learning can directly extrapolate to unseen vertices.

\paragraph{GNN Implementation}
We implemented the four typical GNNs--GCN, GGNN, GAT, GaAN--with PyTorch Geometric 1.5.0.
To compare the performance characteristics of four GNNs side-by-side, we used a unified GNN structure for them: Input Layer $\rightarrow$ GNN Layer 0 $\rightarrow$ GNN Layer 1 $\rightarrow$ Softmax Layer (to prediction).
The structure was popular in the experimental evaluation of GCN \cite{kipf2017_gcn}, GAT \cite{huang2018_gat} and GaAN \cite{zhang2018_gaan}.
Since a GGNN layer requires the input and output hidden feature vectors have the same dimension, for GGNN, we added two MLP layers to transform the dimensions of the input/output feature vectors of the whole GNN: Input Layer $\rightarrow$ MLP $\rightarrow$ GNN Layer 0 $\rightarrow$ GNN Layer 1 $\rightarrow$ MLP $\rightarrow$ Softmax Layer.
We stored the dataset and the model parameters in the memory of GPU.
We conducted all the training on GPU.

\paragraph{Hyper-parameters}
We picked the hyper-parameters of allupdated GNNs according to their popularity in the experimental evaluation in their original papers.
We use $|\boldsymbol{v}|$ to denote the dimension of a vector $\boldsymbol{v}$.
For GCN/GAT/GaAN, $\boldsymbol{h}^0_i = \boldsymbol{x}_i$, i.e. the input feature vector.
$|\boldsymbol{h}^1_i|$ was 64 and $|\boldsymbol{h}^2_i|$ was the number of classes.
For GGNN, $|\boldsymbol{h}^0_i| = |\boldsymbol{h}^1_i| = |\boldsymbol{h}^2_i| = 64$.
We used 8 heads in GAT and GaAN.
$d_{head}=8$ in GAT.
$d_a=d_v=8$ and $d_m=64$ in GaAN.

\subsection{Experimental Scheme}
\label{sec:experimental_scheme}

To find out performance bottleneck in GNN training, we decompose the bottleneck analysis into four questions.
The answers to those questions will give us a more comprehensive view on the performance characteristics of GNN training.
We design extensive experiments to find the answers emperically.
\begin{itemize}
	
	\item[Q1] \emph{How do the hyper-parameters affect the training time and the memory usage of a GNN?}
	\begin{itemize}
		\item Every GNN has a group of hyper-parameters like the number of GNN layers and the dimision of hidden feature vectors. The hyper-parameters affect the training time per epoch and the peak memory usage during the training.
		
		\item To evaluate their effects, we measured how the training time per epoch and the peak memory usage (of GPU) changed as we increased the values of the hyper-parameters.
		
		\item Through the experiments, we want to verify the validity of the computational complexity analysis in \tablename~\ref{tab:gnn_overview_edge} and \tablename~\ref{tab:gnn_overview_vertex}.
		If the complexity analysis is valid, we can analyze the bottleneck theoretically.
	\end{itemize}

	\item[Q2] \emph{Which stage is the most time-consuming stage in GNN training?}
	\begin{itemize}
		\item We can decompose the training time on different levels: layer level, vertex/edge computation level and the basic operator level.
		\item On each level, we breakdown the training time of an epoch into several stages. The most time-consuming stage is the performance bottleneck in time. Optimizing its implementation can significantly reduce the training time.
	\end{itemize}
	\item[Q3] \emph{Which factor prevents us training GNNs on big graphs?}
	\begin{itemize}
		\item 
	\end{itemize}
	\item[Q4] \emph{Can the sampling techniques remove the performance bottleneck in GNN training?}
\end{itemize}

