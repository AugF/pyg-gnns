\section{Related Work}

In this section, we brefily review the graph neural network models survey (GNNs survey), 
graph neural network system(GNN system) and GNNs performance bottleneck analysis work.

\subsection{GNNs survey}
GNNs survey: Zhou et al \cite{zhou2018_gnn_review}, Zhang et al \cite{zhou2018_gnn_review} and Wu et al \cite{comprehensive-survey-wu-2020} 
surveyed existing graph neural works models in different categorizations and viewpoints and provided a comprehensive review about GNNs' model 
architecture, principle and application. Dwived et al \cite{dwivedi2020_benchmark_of_gnn} discussed the performance of different GNN algorithms 
under the graph dataset of different scenes, and gived the key operators of designing effective GNNs in different scenarios. 
However, these efforts are focused on the GNN algorithm itself and accuracy, without a detailed analysis of memory usage and running time.

\subsection{GNN system}
PyG \cite{PyG}, DGL \cite{DGL} designed the graph neural network system based on the message-passing model. 
PyG \cite{PyG} built upon PyTorch and obtained high data throught with sparse GPU accleration by providing dedicated CUDA kernels 
and mini-batch technologies. DGL \cite{DGL} supported a variety of computing backends (Tensorflow, MXNet, PyTorch) and leveraged
fusion kernel techniques, which combine message function with update function to provide further performance improvements compared to PyG \cite{PyG}. 
NeuGraph \cite{ma2019_neugraph} proposed a new programming model, SAGA-NN(Scatter-ApplyEdge-Gather-ApplyVertex with Neural Networks), and achieved excellent
performance in a single machine with mutliple GPUs by using graph computation optimization such as data partitioning management, scheduling and parall mutli-gpu processing.
AliGraph \cite{zhu2019_aligraph} targetd at Attributed Heterogeneous Graph data model, disassembed each layer of GNN into there basic operations: Sampling, Aggregate and Combine,
and leveraged graph parition, spearate storage of attributes and the importance-base vertex caching strategy in storage layer. 
In general, various optimization techniques are used in these GNN systems' design, but whether these optimization techniques match the bottlenecks of GNN performance is a question
worth studying.

\subsection{GNNs performance bottleneck analysis work}
Yan et al \cite{yan2020_analysis_gcns_gpu} compared the performance characteristics of GNN algorithms, Graph Processing and MLP-based Neural Network, provided guidelines in software optimizaton and hardware optimization.
Zhang et \cite{zhang2020_analysis_neugraph} characterized the GNN computation at the inference stage based on SAGA-NN (Scatter-ApplyEdge-Gather-ApplyVertex with Neural Networks) programming model, 
and found that GNN computation has no fixed performance bottlenecks, and believed that each part has the value of optimization. 
Compared to their works, the classification angle of our work is unique, and neither of their works consider the graph scalabilty and the access of sampling technology,
which coverred in this paper.