\section{Conclusion}
\label{sec:conclusion}

In this work, we systematically explore the performance bottlenecks in graph neural network training and inference.
%
We model the existing GNNs with the message-passing framework. 
%
We classify the GNNs according to their edge and vertex calculation complexity to select four typical GNNs for evaluation. 
%
The experimental results validate our complexity analysis.
%
Fixing other hyper-parameters, the training time, inference time, and memory usage increase linearly with each hyper-parameter of the four GNNs.
%
To find out the performance bottlenecks in the training/inference time, we decompose the training/inference time per epoch on different levels.
%
The time breakdown analysis indicates that the edge calculation stage and its related basic operators are the performance bottlenecks for most GNNs.
%
Moreover, the intermediate results produced by the edge calculation stage cause high memory usage, limiting the data scalability.
%
Adopting sampling techniques can reduce the memory usage of training and inference significantly.
%
However, the current implementation of the sampling techniques in PyG brings considerable sampling overheads.
%
The small sampled subgraphs cannot make full use of the computing power of a GPU card either.
% 
Our analysis indicates that the edge calculation stage should be the main target of optimizations.
%
Reducing its memory usage and improving its efficiency can significantly improve the performance of GNN training and inference.
%
Based on the analysis, we propose several potential optimizations for the GNN libraries/systems.
%
We believe that our analysis can help developers to have a better understanding of the characteristics of GNN training and inference.

\section*{Acknowledgements}

This work is funded in part by the National Key R\&D Program of China [grant number 2019YFC1711000], the China NSF Grants [grant number U1811461, 62072230], and the Collaborative Innovation Center of Novel Software Technology and Industrialization; and the program B for Outstanding PhD candidate of Nanjing University.
