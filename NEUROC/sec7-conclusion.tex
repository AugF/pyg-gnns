\section{Conclusion and Future Work}
\label{sec:conclusion}

In this work, we systematically explore the performance bottlenecks in graph neural network training and inference.
%
We model the existing GNNs with the message-passing framework. 
%
We classify the GNNs according to their edge and vertex calculation complexity to select four typical GNNs for evaluation. 
%
The experimental results validate our complexity analysis.
%
Fixing other hyper-parameters, the training time, inference time, and memory usage increase linearly with each hyper-parameter of the four GNNs.
%
To find out the performance bottlenecks in the training/inference time, we decompose the training/inference time per epoch on different levels.
%
The time breakdown analysis indicates that the edge calculation stage and its related basic operators are the performance bottlenecks for most GNNs.
%
Moreover, the intermediate results produced by the edge calculation stage cause high memory usage, limiting the data scalability.
%
Adopting sampling techniques can reduce the memory usage of training and inference significantly, without sacrificing accuracy. 
%
However, the current implementation of the sampling techniques in PyG brings considerable sampling overheads.
%
The small sampled subgraphs cannot make full use of the computing power of a GPU card either.
% 
Our analysis indicates that the edge calculation stage should be the main target of optimizations.
%
Reducing its memory usage and improving its efficiency can significantly improve the performance of GNN training and inference.
%
Based on the analysis, we propose several potential optimizations for the GNN libraries/systems.
%
We believe that our analysis can help developers to have a better understanding of the characteristics of GNN training and inference.

In this work, we analyze performance bottlenecks in a \emph{single-GPU} environment on \emph{static} graphs with the \emph{message-passing} framework.
%
Performance bottlenecks in multi-GPU/distributed GNN training/inference, dynamic graphs and other GNN frameworks are also worth studying.
%
In the future, we plan to extend our analysis of performance bottlenecks in GNN training/inference with the following directions:
%
\begin{enumerate}
    \item \emph{Multi-GPU or distributed environment.}
    %
    To handle large-scale graph datasets, multi-GPU and distributed training/inference are necessary.
    %
    These methods will inevitably introduce overheads such as inter-GPU and inter-machine communication. 
    %
    How these overheads affect performance bottlenecks is worthy to focus on.
    %
    \item \emph{Spatial-temporal graph datasets.}
    %
    Spatial-temporal graphs have dynamic topology structures.
    %
    They appear in a variety of applications like traffic speed forecasting \cite{li2018_DCRNN} and human action recognition \cite{yan2018_STGCN}.
    %
    %Learning hidden patterns from spatial-temporal graphs become increasingly important.
    %
    Many new GNNs are proposed to handle this kind of graph.
    %
    How the performance bottlenecks of these GNNs are different from the classic GNNs is also worthy of in-depth study.
    %
    %Whether spatial-temporal graphs will affect performance is worthy of our attention.
    %
    \item \emph{Other GNN frameworks.}
    %
    In this work, we analyzed with the popular message-passing framework.
    %
    Some emerging GNN learning systems also adopt different GNN frameworks like SAGA framework \cite{ma2019_neugraph} and edge-centric framework \cite{he2019_EnGN}.
    %
    Whether different frameworks lead to different performance bottlenecks is worth discussing.
\end{enumerate}
\section*{Acknowledgements}

This work is funded in part by the National Key R\&D Program of China (2019YFC1711000), the China NSF Grants (No. U1811461, 62072230), the Collaborative Innovation Center of Novel Software Technology and Industrialization, Alibaba Innovative Resarch Project, and the program B for outstanding PhD candidate of Nanjing University.

