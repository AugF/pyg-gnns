\section{Conclusion}
\label{sec:conclusion}

In this work, we systematically explore the performance bottlenecks in graph neural network training and inference.
%
We model the existing GNNs with the message-passing framework. 
%
We classify the GNNs according to their edge and vertex calculation complexity to select four typical GNNs for evaluation. 
%
The experimental results validate our complexity analysis.
%
Fixing other hyper-parameters, the training time, inference time, and memory usage increase linearly with each hyper-parameter of the four GNNs.
%
To find out the performance bottlenecks in the training/inference time, we decompose the training/inference time per epoch on different levels.
%
The time breakdown analysis indicates that the edge calculation stage and its related basic operators are the performance bottlenecks for most GNNs.
%
Moreover, the intermediate results produced by the edge calculation stage cause high memory usage, limiting the data scalability.
%
Adopting sampling techniques can reduce the memory usage of training and inference significantly, without sacrificing accuracy. 
%
However, the current implementation of the sampling techniques in PyG brings considerable sampling overheads.
%
The small sampled subgraphs cannot make full use of the computing power of a GPU card either.
% 
Our analysis indicates that the edge calculation stage should be the main target of optimizations.
%
Reducing its memory usage and improving its efficiency can significantly improve the performance of GNN training and inference.
%
Based on the analysis, we propose several potential optimizations for the GNN libraries/systems.
%
We believe that our analysis can help developers to have a better understanding of the characteristics of GNN training and inference.

% start, added by wangyunpan, 2021-2-21
In the future, we will focus on but not limited to the following directions:
%
\begin{enumerate}
    \item \emph{Performance bottlenecks in training/inference with multi-GPUs or distributed environments.}
    %
    For massive data in the real world, multi-GPUs and distributed training/inference are often the more common methods\cite{zhang2020_analysis_neugraph, zhu2019_aligraph}.
    %
    These methods will inevitably introduce overheads such as communication between GPUs (inter-machines). 
    %
    How these overheads affect performance bottlenecks is worthy to focus on.
    %
    \item \emph{Impacts of different programming models on performance bottlenecks.}
    %
    Our work is based on the mainstream programming model---messaging framework.
    %
    However, some GNNs systems also proposed some new programming models.
    %
    For example, NeuGraph\cite{zhang2020_analysis_neugraph} proposed the SAGA framework, 
    which includes four stages: Scatter, ApplyEdge, Gather, and ApplyVertex;
    %
    EuGN\cite{he2019_EnGN} provides an edge-centric programming model.
    %
    Whether different programming models will lead to changes in performance bottlenecks is worth discussing.
    %
    \item \emph{Impacts of spatial-temporal graph on performance bottlenecks.}
    %
    Spatial-temporal graphs appear in a variety of applications such as traffic speed forecasting\cite{li2018_DCRNN}, 
    driver maneuver anticipation\cite{jain2016_SRNN}, and human action recognition\cite{yan2018_STGCN}.
    %
    Learning hidden patterns from spatial-temporal graphs becomes increasingly important.
    %
    Many GNNs are also proposed\cite{li2018_DCRNN, jain2016_SRNN, yan2018_STGCN, seo2018_SSGCN, yu2018_STGCN}.
    %
    Whether spatial-temporal graphs will affect performance is worthy of our attention.
    %
\end{enumerate}
% end, added by wangyunpan, 2021-2-21
\section*{Acknowledgements}

This work is funded in part by the National Key R\&D Program of China (2019YFC1711000), the China NSF Grants (No. U1811461, 62072230), the Collaborative Innovation Center of Novel Software Technology and Industrialization, Alibaba Innovative Resarch Project, and the program B for outstanding PhD candidate of Nanjing University.

