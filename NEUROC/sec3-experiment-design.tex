\section{Evaluation Design}
\label{sec:experimental_design}

We design a series of experiments to explore the performance bottleneck in training graph neural networks.
We first introduce our experimental setting in Section~\ref{sec:experimental_env} and then give out our experimental scheme in Section~\ref{sec:experimental_scheme}.
The evaluation results are presented and analyzed later in Section~\ref{sec:experiment_results}.

\subsection{Experimental Setting}
\label{sec:experimental_env}

\paragraph{Experimental Environment}
All the experiments were conducted in a CentOS 7 server with the Linux kernel version 3.10.0.
The server had 40 cores and 90 GB main memory.
The server was equipped with an NVIDIA Tesla T4 GPU card with 16GB GDDR6 memory.
For the software environment, we adopted Python 3.7.7, PyTorch 1.5.0, and CUDA 10.1.
We implemented all GNNs with PyG 1.5.0\footnote{https://pytorch-geometric.readthedocs.io/en/1.5.0/index.html}.

\paragraph{Datasets}

We used six real-world graph datasets as listed in \tablename~\ref{tab:dataset_overview} that were popular in the GNN accuracy evaluation \cite{yang2016_revisiting_semisupervised, zeng2020_graphsaint, shchur2018_pitfall_of_gnn}.
%
For directed graphs, PyG converts them into undirected ones during data loading.
%
Thus, the average degree of a directed graph $\bar{d}=\frac{2|\mathcal{E}|}{|\mathcal{V}|}$.
%
For an undirected graph, the average degree is defined as $\bar{d}=\frac{|\mathcal{E}|}{|\mathcal{V}|}$.
%
For the \texttt{cam} dataset, since its vertices were not associated with feature vectors, we generated random dense feature vectors for it and excluded it from accuracy evaluation.
%
We also used random graphs generated by the R-MAT graph generator \cite{rmat-generator} in some experiments, to explore the effects of graph topological characteristics (like average degrees) on performance bottlenecks.
%
Input feature vectors of random graphs were random dense vectors with the dimension of 32.
%
Vertices of random graphs were classified into 10 classes randomly.

\begin{table}[H]
    \centering
    \begin{tabular}{cccccccc}
        \toprule
        Dataset                                                 & $|\mathcal{V}|$ & $|\mathcal{E}|$ & $\bar{d}$ & $dim(\boldsymbol{v})$ & \#Class & Directed \\
        \midrule
        pubmed (\texttt{pub}) \cite{yang2016_revisiting_semisupervised}  & 19,717          & 44,324          & 4.5       & 500                   & 3       & Yes      \\
        amazon-photo (\texttt{amp}) \cite{shchur2018_pitfall_of_gnn}     & 7,650           & 119,081         & 31.1      & 745                   & 8       & Yes      \\
        amazon-computers (\texttt{amc}) \cite{shchur2018_pitfall_of_gnn} & 13,752          & 245,861         & 35.8      & 767                   & 10      & Yes      \\
        coauthor-physics (\texttt{cph}) \cite{shchur2018_pitfall_of_gnn} & 34,493          & 247,962         & 14.4      & 8415                  & 5       & Yes      \\
        flickr (\texttt{fli}) \cite{zeng2020_graphsaint}                 & 89,250          & 899,756         & 10.1      & 500                   & 7       & No       \\
        com-amazon (\texttt{cam}) \cite{yang2012_defining}               & 334,863         & 925,872         & 2.8       & 32                    & 10      & No       \\
        \bottomrule
    \end{tabular}
    \caption{Dataset overview. $\bar{d}$ represents the average vertex degree. $dim(\boldsymbol{v})$ is the dimension of the input feature vector.}
    \label{tab:dataset_overview}
\end{table}

\paragraph{Learning Task}
We used the node classification as the target task in GNNs due to its popularity in real-world applications.
We trained GNNs in the semi-supervised learning setting.
All vertices and their input feature vectors were used, but only a part of the vertices were attached with labels during the training and they were used to calculate the loss and gradients.
The vertices with unseen labels were used in the evaluation phase to evaluate the accuracy of the current parameters.
%Since model parameters of GNNs were not restricted by the topological structure of $\mathcal{G}$, the model learned from the semi-supervised learning can directly extrapolate to unseen vertices.

\paragraph{GNN Implementation}
We implemented the four typical GNNs: GCN, GGNN, GAT, and GaAN.
To compare performance characteristics of the four GNNs side-by-side, we used a unified GNN structure for them: Input Layer $\rightarrow$ GNN Layer 0 $\rightarrow$ GNN Layer 1 $\rightarrow$ Softmax Layer (to prediction).
The structure was popular in the experimental evaluation of GCN \cite{kipf2017_gcn}, GAT \cite{huang2018_gat}, and GaAN \cite{zhang2018_gaan}.
Since a GGNN layer requires the input and output hidden vectors have the same dimension, we added two multi-layer perceptron (MLP) layers to transform the dimensions of the input/output feature vectors: Input Layer $\rightarrow$ MLP $\rightarrow$ GGNN Layer 0 $\rightarrow$ GGNN Layer 1 $\rightarrow$ MLP $\rightarrow$ Softmax Layer.
Unless otherwise specified, we stored the dataset and the model parameters on the GPU side.
The GNN training was conducted also on the GPU side.

\paragraph{Hyper-parameters}
\label{sec:hyper-parameters}

We used $dim(\boldsymbol{x})$ to denote the dimension of a vector $\boldsymbol{x}$.
%
We picked the hyper-parameters of GNNs according to their popularity in their references \cite{kipf2017_gcn, li2015_ggnn, huang2018_gat, zhang2018_gaan}.
%
Unless otherwise mentioned, we used the same set of hyper-parameters for all the datasets. 
%
Some hyper-parameters (like dimensions of hidden vectors) were common in the four GNNs and we set them to the same values.
%
For GCN/GAT/GaAN, we set $\boldsymbol{h}^0_x = \boldsymbol{v}_x$, $dim(\boldsymbol{h}^1_x)=64$, and $dim(\boldsymbol{h}^2_x)=\#Class$ (the number of classes in the dataset).
%
For GAT, the first GAT layer contained 8 heads, and the attention vector of each head had a dimension of 8.
%
The first GAT layer merged attention vectors of 8 heads by concatenating.
%
The second GAT layer used a single head with a dimension of $\#Class$.
%
For GGNN, we set $dim(\boldsymbol{h}^0_x) = dim(\boldsymbol{h}^1_x) = dim(\boldsymbol{h}^2_x) = 64$.
%
We used 8 heads in the both GaAN layers with $d_a=d_v=8$, and $d_m=64$.

We used Adam \cite{diederik2015_adam} as the gradient descent optimizer with a learning rate of 0.01 and a weight decay of 1e-5.
%
We initialized weights in GNNs using the method described in \cite{xavier2010_glorot} and initialized bias as zeros.
%
Without otherwise mentioned, in the experiments related to accuracy evaluation, we trained all GNNs for a maximum of 100k epochs and used the early stopping criterion according to \cite{shchur2018_pitfall_of_gnn}:
%
the training was stopped if the validation loss did not improve for 50 epochs.
%
We chose the model parameters that achieved the lowest validation loss as the final model parameters.
%
The accuracy on the test set was evaluated with the final model.

\paragraph{Sampling Techniques}

We picked the neighbor sampler from GraphSAGE \cite{hamilton2017_graphsage} and the cluster sampler from ClusterGCN \cite{chiang2019_cluster_gcn} as the typical sampling techniques.
%
For the neighbor sampler, we set the neighborhood sample sizes of the GNN layer 0 and the GNN layer 1 to 10 and 25, respectively.
%
We set the default batch size to 512, according to \cite{hamilton2017_graphsage}.
%
For the cluster sampler, we partitioned the input graph into 1500 partitions and used 20 partitions per batch, according to \cite{chiang2019_cluster_gcn}.

\subsection{Experimental Scheme}
\label{sec:experimental_scheme}

To find out performance bottlenecks in GNN training, we conducted the experimental analysis with four questions.
%
The answers to those questions will give us a more comprehensive view of performance characteristics of GNN training.

\begin{itemize}

    \item[Q1] \emph{How do the hyper-parameters affect the training/inference time, memory usage and accuracy of a GNN?} (Section~\ref{sec:effects_of_hyper-parameters_on_performance})

          Every GNN had a group of hyper-parameters, such as the number of GNN layers and the dimensions of hidden feature vectors.
          %
          The hyper-parameters affected the processing time per epoch and the peak memory usage during training and inference.
          %
          They also affect the accuracy of a GNN.
          %
          Larger hyper-parameter values usually mean more model parameters and more complex models.
          %
          To evaluate their effects, we measured how the training/inference time per epoch, the peak memory usage (of the GPU), and the accuracy on the test set changed as we increased the values of the hyper-parameters.
          %
          Through the experiments, we verified the validity of the time complexity analysis in \tablename~\ref{tab:gnn_overview_edge} and \tablename~\ref{tab:gnn_overview_vertex}.
          %
          The complexity analysis allowed us to analyze performance bottlenecks theoretically.

    \item[Q2] \emph{Which stage is the most time-consuming stage in GNN training and inference?} (Section~\ref{sec:training_time_breakdown})

          We decomposed the GNN training time on different levels: the layer level, the edge/vertex calculation level, and the basic operator level.
          %
          On each level, we decomposed the training time of an epoch into several stages.
          %
          The most time-consuming stage was the performance bottleneck.
          %
          Optimizing its implementation will significantly reduce the training time.
          %
          For GNN inference, we conducted similar experimental analysis.
          %
          We discussed the differences in performance characteristics between training and inference.

    \item[Q3] \emph{Which consumes most of memory in GNN training?} (Section~\ref{sec:memory_usage_analysis})

          The limited memory capacity of a GPU prevents us from training GNNs on big graphs.
          %
          We measured the peak memory usage during GNN training under different graph scales, input feature dimensions, and average degrees.
          %
          Based on the results, we analyzed which was the most memory-consuming component in a GNN.
          %
          Reducing its memory usage will enable us to train GNNs on bigger graphs under the same memory capacity.

    \item[Q4] \emph{Can sampling techniques remove performance bottlenecks in GNN training? Does the use of sampling techniques in training affect the model accuracy?} (Section~\ref{sec:effects_of_sampling_techniques_on_performance})

          Theoretically, sampling techniques can significantly reduce the number of graph neurons that participate in the training of a batch.
          %
          Consequently, the processing time and memory usage per batch should also decrease.
          %
          The model accuracy of GNNs may also be affected.
          %
          To validate effectiveness of sampling techniques, we measured the training time, peak memory usage and test accuracy under different batch sizes.
          %
          If sampling techniques are effective, they will be the keys to conduct GNN training on very big graphs.
          %
          If they are not effective, we want to find out which impairs its efficiency.
          %

\end{itemize}