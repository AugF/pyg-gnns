\section{Insights}
\label{sec:insights}

Through the extensive experiments, we propose the following key findings and suggestions for how to optimize the performance for GNN training and inference.

\begin{enumerate}
    \item \emph{The time complexity in \tablename~\ref{tab:gnn_overview_edge} and \tablename~\ref{tab:gnn_overview_vertex} points out performance bottlenecks theoretically.}
          The experimental results validate the time complexity analysis.
          The time complexity points out where the bottleneck comes from.
          Optimization should focus on complex operations in the messaging function $\phi$ and the vertex updating function $\gamma$.

    \item \emph{The computational cost of a GNN layer is mainly affected by the dimensions of the input and output hidden vectors.}
          Theoretically and empirically, the training and inference time and the memory usage of a GNN layer both increase \emph{linearly} with the dimensions of the input/output hidden vectors separately.
          GNNs are friendly to high-dimensional scenarios. 
          Algorithm engineers can use high-dimensional feature vectors to improve the expressive power of a GNN without worrying exponential growth in the training/inference time and memory usage
    \item \emph{Performance optimizations should focus on improving the efficiency of the edge calculation stage.}
          The edge calculation stage is the most time-consuming stage in most GNNs.
          \begin{itemize}
              \item If the complexity of the messaging function $\phi$ is high, the implementation of $\phi$ is critical to performance.
                    Improving its efficiency can significantly reduce training/inference time.
                    For example, the attention mechanism in GNNs (like GAT and GaAN) requires an extra sub-layer to calculate the attention weight of each edge.
                    Implementing the attention mechanism with specially optimized basic operators on the GPU side is a potential optimization direction.
              \item If the complexity of $\phi$ is low, the efficiency of the collection step and the aggregation step becomes critical.
                    The existing GNN libraries \cite{DGL, PyG, ma2019_neugraph} already introduce the \emph{fused} operator to improve their efficiency.
                    When the messaging function $\phi$ is an assignment or a scalar multiplication of the input hidden vector of the source vertex, the libraries replace the collection, messaging, and aggregation steps with a single fused operator.
                    The fused operator calculates the aggregated vectors directly from the input hidden vectors, minimizing the memory footprints and overlapping the memory accessing with computation.
                    In this way, it significantly reduces the training/inference time of GNNs with low edge calculation complexity (like GCN) \cite{yan2020_characterizing_gcn, zhang2020_analysis_neugraph}.
                    However, the applicable condition of the fused operator is very restricted.
                    It does not work for $\phi$ with more complex operations like matrix multiplication.
                    A potential optimization is to develop composite CUDA kernels that can read the input hidden vectors and aggregate message vectors on the fly, without materializing the parameter vectors and the message vectors.
         \end{itemize}
    \item \emph{The high memory usage caused by the intermediate results of the edge calculation stage limits the data scalability of GNN training/inference.}
          The memory expansion ratios of the typical GNNs are very high, making GPUs unable to handle big graphs.
          One solution is to distribute the dataset among several GPUs and frequently swap parts of the dataset between GPUs and the main memory \cite{ma2019_neugraph}.
          Another possible solution \cite{chen2016_training_deep} comes from the deep neural network training. % checkpoints only work for GNN training.
          It only checkpoints key intermediate results during the forward propagation and re-calculates the missing results on demand during the backpropagation.
          Implementing the checkpoint mechanism is another potential optimization for GNN training.

    \item \emph{Sampling techniques can significantly reduce the memory usage per batch, but its implementation is still inefficient}.
          %
          The sampling techniques are effective under small batch sizes for both training and inference.
          %
          With proper batch sizes, the accuracy of the sample-based training can be close to the accuracy of the full-batch training.
          %
          However, the current implementation of the sampling techniques in PyG brings considerable overheads when the batch size is small.
          %
          Improving the efficiency of the sampling techniques is a potential optimization direction.
          %
          The small sampled subgraphs cannot make full use of the computing power of the GPU either.
          %
          How to improve the GPU utilization under small batch sizes is another problem to solve.
          One possible solution is to train multiple batches asynchronously on the same GPU and use the asynchronous stochastic gradient descent to speed up the converge.
\end{enumerate}
