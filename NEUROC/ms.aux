\relax 
\providecommand\hyper@newdestlabel[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{elsarticle-num}
\emailauthor{{wangzhaokang, wangyp}@smail.nju.edu.cn, {cfyuan, gurong, yhuang}@nju.edu.cn}{Zhaokang Wang, Yunpan Wang, Chunfeng Yuan, Rong Gu$^*$ \corref {correspondingauthor1}, Yihua Huang$^*$ \corref {correspondingauthor2}}
\Newlabel{correspondingauthor}{1}
\citation{kipf2017_gcn,defferrad2016_chebnet,li2018_agcn,li2015_ggnn,hamilton2017_graphsage,huang2018_gat,zhang2018_gaan}
\citation{zhou2018_gnn_review,zhang2018_gnn_survey,comprehensive-survey-wu-2020}
\citation{qi2018_learning_humanobject}
\citation{wang2020_hierarchical_human_parsing}
\citation{wang2019_zeroshot_video}
\citation{PyG,DGL,ma2019_neugraph,zhu2019_aligraph,PGL}
\citation{PyG}
\citation{ma2019_neugraph}
\citation{PGL}
\citation{DGL}
\citation{DGL}
\citation{ma2019_neugraph}
\citation{ma2019_neugraph}
\citation{yan2020_characterizing_gcn}
\citation{zhang2020_analysis_neugraph}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{kipf2017_gcn}
\citation{li2015_ggnn}
\citation{huang2018_gat}
\citation{zhang2018_gaan}
\@writefile{toc}{\contentsline {paragraph}{Outline}{2}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Review of Graph Neural Networks}{3}{section.2}\protected@file@percent }
\newlabel{sec:review_of_gnns}{{2}{3}{Review of Graph Neural Networks}{section.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Notations\relax }}{3}{table.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:notations}{{1}{3}{Notations\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Structure of Graph Neural Networks}{3}{subsection.2.1}\protected@file@percent }
\citation{bryan2014_deepwalk}
\citation{aditya2016_node2vec}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Structure of a typical graph neural network. (a) Demo GNN, (b) Demo graph. The target application is the node classification. The demo GNN has two GNN layers.\relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:general_structure_of_gnn}{{1}{4}{Structure of a typical graph neural network. (a) Demo GNN, (b) Demo graph. The target application is the node classification. The demo GNN has two GNN layers.\relax }{figure.caption.3}{}}
\citation{gilmer_messgae_passing}
\citation{PyG}
\citation{DGL}
\citation{huang2018_gat}
\citation{zhang2018_gaan}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Graph Neuron and Message-passing Model}{5}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Graph neuron of $v_3$ in the GNN layer $l$ with the demo graph $\mathcal  {G}$ in Figure~\ref  {fig:general_structure_of_gnn}b. $\phi ^l$/$\Sigma ^l$/$\gamma ^l$ are the messaging/aggregation/updating functions in the message-passing model, respecitvely.\relax }}{5}{figure.caption.4}\protected@file@percent }
\newlabel{fig:graph_neuron_structure}{{2}{5}{Graph neuron of $v_3$ in the GNN layer $l$ with the demo graph $\mathcal {G}$ in \figurename ~\ref {fig:general_structure_of_gnn}b. $\phi ^l$/$\Sigma ^l$/$\gamma ^l$ are the messaging/aggregation/updating functions in the message-passing model, respecitvely.\relax }{figure.caption.4}{}}
\citation{defferrad2016_chebnet}
\citation{kipf2017_gcn}
\citation{li2018_agcn}
\citation{hamilton2017_graphsage}
\citation{hamilton2017_graphsage}
\citation{duvenaud2015_neural_fps}
\citation{han2018_sse}
\citation{li2015_ggnn}
\citation{huang2018_gat}
\citation{zhang2018_gaan}
\citation{defferrad2016_chebnet}
\citation{kipf2017_gcn}
\citation{li2018_agcn}
\citation{hamilton2017_graphsage}
\citation{hamilton2017_graphsage}
\citation{duvenaud2015_neural_fps}
\citation{han2018_sse}
\citation{li2015_ggnn}
\citation{huang2018_gat}
\citation{zhang2018_gaan}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Typical graph neural networks and their edge calculation functions. $d_{in}$ and $d_{out}$ are the dimensions of the input and output hidden vectors, respectively. Blue variables are model parameters to learn. $\delta $ is the activation function. \relax }}{6}{table.caption.5}\protected@file@percent }
\newlabel{tab:gnn_overview_edge}{{2}{6}{Typical graph neural networks and their edge calculation functions. $d_{in}$ and $d_{out}$ are the dimensions of the input and output hidden vectors, respectively. Blue variables are model parameters to learn. $\delta $ is the activation function. \relax }{table.caption.5}{}}
\citation{kipf2017_gcn}
\citation{li2015_ggnn}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Typical graph neural networks and their vertex calculation functions. $d_{in}$ and $d_{out}$ are the dimensions of the input and output hidden vectors, respectively. Blue variables are model parameters to learn. In Neural FPs, $\leavevmode {\color  {blue}\boldsymbol  {W}}^{l, |\mathcal  {N}(i)|}$ is the weight matrix for vertices with degree $|\mathcal  {N}(i)|$ at the layer $l$. $\delta $ is the activation function. \relax }}{7}{table.caption.6}\protected@file@percent }
\newlabel{tab:gnn_overview_vertex}{{3}{7}{Typical graph neural networks and their vertex calculation functions. $d_{in}$ and $d_{out}$ are the dimensions of the input and output hidden vectors, respectively. Blue variables are model parameters to learn. In Neural FPs, $\textcolor {blue}{\boldsymbol {W}}^{l, |\mathcal {N}(i)|}$ is the weight matrix for vertices with degree $|\mathcal {N}(i)|$ at the layer $l$. $\delta $ is the activation function. \relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Representative GNNs}{7}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}GCN (Low Vertex \& Low Edge Complexity)}{7}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}GGNN (High Vertex \& Low Edge Complexity)}{7}{subsubsection.2.3.2}\protected@file@percent }
\citation{huang2018_gat}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Complexity quadrants of typical GNNs. We compare the complexity according to the number of sub-layers, the Big-O notation, and the number of parameters to train.\relax }}{8}{figure.caption.7}\protected@file@percent }
\newlabel{fig:gnn_complexity_quadrant}{{3}{8}{Complexity quadrants of typical GNNs. We compare the complexity according to the number of sub-layers, the Big-O notation, and the number of parameters to train.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}GAT (Low Vertex \& High Edge Complexity)}{8}{subsubsection.2.3.3}\protected@file@percent }
\newlabel{eq:GAT-sub-layer-1}{{2}{8}{GAT (Low Vertex \& High Edge Complexity)}{equation.2.2}{}}
\newlabel{eq:GAT-sub-layer-2}{{3}{8}{GAT (Low Vertex \& High Edge Complexity)}{equation.2.3}{}}
\citation{zhang2018_gaan}
\citation{chiang2019_cluster_gcn}
\citation{hamilton2017_graphsage,ying2018_pinsage,chen2018_fastgcn,chen2018_sgcn,zeng2018_aesg,chiang2019_cluster_gcn,zeng2020_graphsaint}
\citation{hamilton2017_graphsage,ying2018_pinsage,chen2018_fastgcn,chen2018_sgcn,huang2018_adap}
\citation{hamilton2017_graphsage}
\citation{zeng2018_aesg,chiang2019_cluster_gcn,zeng2020_graphsaint}
\citation{chiang2019_cluster_gcn}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}GaAN (High Vertex \& High Edge Complexity)}{9}{subsubsection.2.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Sampling Techniques}{9}{subsection.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Training a GNN with sampling techniques. The faded graph neurons and their connections are inactivated.\relax }}{9}{figure.caption.8}\protected@file@percent }
\newlabel{fig:gnn_sampling}{{4}{9}{Training a GNN with sampling techniques. The faded graph neurons and their connections are inactivated.\relax }{figure.caption.8}{}}
\citation{yang2016_revisiting_semisupervised,zeng2020_graphsaint,shchur2018_pitfall_of_gnn}
\citation{rmat-generator}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Inference with GNNs}{10}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Evaluation Design}{10}{section.3}\protected@file@percent }
\newlabel{sec:experimental_design}{{3}{10}{Evaluation Design}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Experimental Setting}{10}{subsection.3.1}\protected@file@percent }
\newlabel{sec:experimental_env}{{3.1}{10}{Experimental Setting}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Experimental Environment}{10}{section*.9}\protected@file@percent }
\citation{yang2016_revisiting_semisupervised}
\citation{shchur2018_pitfall_of_gnn}
\citation{shchur2018_pitfall_of_gnn}
\citation{shchur2018_pitfall_of_gnn}
\citation{zeng2020_graphsaint}
\citation{yang2012_defining}
\citation{kipf2017_gcn}
\citation{huang2018_gat}
\citation{zhang2018_gaan}
\citation{kipf2017_gcn,li2015_ggnn,huang2018_gat,zhang2018_gaan}
\citation{hamilton2017_graphsage}
\citation{chiang2019_cluster_gcn}
\citation{hamilton2017_graphsage}
\citation{chiang2019_cluster_gcn}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Dataset overview. $\bar  {d}$ represents the average vertex degree. $dim(\boldsymbol  {v})$ is the dimension of the input feature vector.\relax }}{11}{table.caption.11}\protected@file@percent }
\newlabel{tab:dataset_overview}{{4}{11}{Dataset overview. $\bar {d}$ represents the average vertex degree. $dim(\boldsymbol {v})$ is the dimension of the input feature vector.\relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Datasets}{11}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Learning Task}{11}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{GNN Implementation}{11}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hyper-parameters}{11}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sampling Techniques}{11}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Experimental Scheme}{11}{subsection.3.2}\protected@file@percent }
\newlabel{sec:experimental_scheme}{{3.2}{11}{Experimental Scheme}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Evaluation Results and Analysis}{12}{section.4}\protected@file@percent }
\newlabel{sec:experiment_results}{{4}{12}{Evaluation Results and Analysis}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Effects of Hyper-parameters on Performance}{12}{subsection.4.1}\protected@file@percent }
\newlabel{sec:effects_of_hyper-parameters_on_performance}{{4.1}{12}{Effects of Hyper-parameters on Performance}{subsection.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Distribution of the wall-clock training time of 50 epoches on different datasets. GaAN crashed due to out of memory exception on the \texttt  {cph} dataset.\relax }}{13}{figure.caption.16}\protected@file@percent }
\newlabel{fig:exp_absolute_training_time}{{5}{13}{Distribution of the wall-clock training time of 50 epoches on different datasets. GaAN crashed due to out of memory exception on the \texttt {cph} dataset.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Effects of hyper-parameters on the vertex/edge calculation time of GCN.\relax }}{13}{figure.caption.17}\protected@file@percent }
\newlabel{fig:exp_hyperparameter_on_vertex_edge_phase_time_gcn}{{6}{13}{Effects of hyper-parameters on the vertex/edge calculation time of GCN.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Effects of hyper-parameters on the vertex/edge calculation time of GGNN.\relax }}{13}{figure.caption.18}\protected@file@percent }
\newlabel{fig:exp_hyperparameter_on_vertex_edge_phase_time_ggnn}{{7}{13}{Effects of hyper-parameters on the vertex/edge calculation time of GGNN.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Effects of hyper-parameters on the vertex/edge calculation time of GAT.\relax }}{14}{figure.caption.19}\protected@file@percent }
\newlabel{fig:exp_hyperparameter_on_vertex_edge_phase_time_gat}{{8}{14}{Effects of hyper-parameters on the vertex/edge calculation time of GAT.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Effects of hyper-parameters on the vertex/edge calculation time of GaAN.\relax }}{14}{figure.caption.20}\protected@file@percent }
\newlabel{fig:exp_hyperparameter_on_vertex_edge_phase_time_gaan}{{9}{14}{Effects of hyper-parameters on the vertex/edge calculation time of GaAN.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Effects of hyper-parameters on the peak GPU memory usage during the training, excluding the memory used by the dataset and the model parameters.\relax }}{16}{figure.caption.21}\protected@file@percent }
\newlabel{fig:exp_hyperparameter_memory_usage}{{10}{16}{Effects of hyper-parameters on the peak GPU memory usage during the training, excluding the memory used by the dataset and the model parameters.\relax }{figure.caption.21}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {GCN}}}{16}{subfigure.10.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {GGNN}}}{16}{subfigure.10.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {GAT}}}{16}{subfigure.10.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {GaAN}}}{16}{subfigure.10.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary}{16}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Training Time Breakdown}{16}{subsection.4.2}\protected@file@percent }
\newlabel{sec:training_time_breakdown}{{4.2}{16}{Training Time Breakdown}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Layer Level}{16}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Training time breakdown on the layer level. The training time of each layer included the time spent on the forward, backward and evaluation phases. Each layer was further decomposed into the vertex and the edge calculation stages.\relax }}{17}{figure.caption.23}\protected@file@percent }
\newlabel{fig:exp_vertex_edge_cal_proportion}{{11}{17}{Training time breakdown on the layer level. The training time of each layer included the time spent on the forward, backward and evaluation phases. Each layer was further decomposed into the vertex and the edge calculation stages.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {GCN}}}{17}{subfigure.11.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {GGNN}}}{17}{subfigure.11.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {GAT}}}{17}{subfigure.11.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {GaAN}}}{17}{subfigure.11.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Effects of the average degree on the time proportion of the edge/vertex calculation. Graphs were generated with the R-MAT generator by fixing the number of vertices as 50,000. \relax }}{18}{figure.caption.24}\protected@file@percent }
\newlabel{fig:exp_avg_degree_on_vertex_edge_cal_time}{{12}{18}{Effects of the average degree on the time proportion of the edge/vertex calculation. Graphs were generated with the R-MAT generator by fixing the number of vertices as 50,000. \relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {GCN}}}{18}{subfigure.12.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {GGNN}}}{18}{subfigure.12.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {GAT}}}{18}{subfigure.12.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {GaAN}}}{18}{subfigure.12.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Step Level in Edge Calculation}{18}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Step decomposition of the edge calculation stage of the GNN layer $l$.\relax }}{18}{figure.caption.25}\protected@file@percent }
\newlabel{fig:steps_in_edge_calculation}{{13}{18}{Step decomposition of the edge calculation stage of the GNN layer $l$.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Training time breakdown of the edge calculation stage.\relax }}{19}{figure.caption.26}\protected@file@percent }
\newlabel{fig:exp_edge_calc_decomposition}{{14}{19}{Training time breakdown of the edge calculation stage.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {GCN}}}{19}{subfigure.14.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {GGNN}}}{19}{subfigure.14.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {GAT}}}{19}{subfigure.14.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {GaAN}}}{19}{subfigure.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Operator Level}{19}{subsubsection.4.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Top 5 time-consuming basic operators of typical GNNs. The time proportion of each basic operator was averaged over all datasets with the error bar indicating the maximum and the minimum.\relax }}{20}{figure.caption.27}\protected@file@percent }
\newlabel{fig:exp_top_basic_ops}{{15}{20}{Top 5 time-consuming basic operators of typical GNNs. The time proportion of each basic operator was averaged over all datasets with the error bar indicating the maximum and the minimum.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {GCN}}}{20}{subfigure.15.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {GGNN}}}{20}{subfigure.15.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {GAT}}}{20}{subfigure.15.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {GaAN}}}{20}{subfigure.15.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{GCN}{20}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{GGNN}{20}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{GAT}{20}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{GaAN}{20}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of Training Time Breakdown}{21}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Memory Usage Analysis}{21}{subsection.4.3}\protected@file@percent }
\newlabel{sec:memory_usage_analysis}{{4.3}{21}{Memory Usage Analysis}{subsection.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Memory usage of each phase during the GNN training. Dataset: \texttt  {amp}.\relax }}{21}{figure.caption.33}\protected@file@percent }
\newlabel{fig:exp_memory_usage_stage_amp}{{16}{21}{Memory usage of each phase during the GNN training. Dataset: \texttt {amp}.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Computation graph of the vertex updating function $\gamma $ of GGNN.\relax }}{21}{figure.caption.34}\protected@file@percent }
\newlabel{fig:ggnn_vertex_func_computation_graph}{{17}{21}{Computation graph of the vertex updating function $\gamma $ of GGNN.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Memory expansion ratios of typical GNNs.\relax }}{22}{figure.caption.35}\protected@file@percent }
\newlabel{fig:exp_memory_expansion_ratio}{{18}{22}{Memory expansion ratios of typical GNNs.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Memory expansion ratio under different dimensions of input feature vectors. Dataset: \texttt  {cam}.\relax }}{22}{figure.caption.36}\protected@file@percent }
\newlabel{fig:exp_memory_expension_ratio_input_feature_dimension}{{19}{22}{Memory expansion ratio under different dimensions of input feature vectors. Dataset: \texttt {cam}.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Memory usage under different average degrees. The random graphs were generated by fixing the number of vertices at 10K and the dimension of input feature vectors at 32.\relax }}{23}{figure.caption.37}\protected@file@percent }
\newlabel{fig:exp_memory_expansion_ratio_input_graph_number_of_edges}{{20}{23}{Memory usage under different average degrees. The random graphs were generated by fixing the number of vertices at 10K and the dimension of input feature vectors at 32.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Peak memory usage}}}{23}{subfigure.20.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Memory expansion ratio}}}{23}{subfigure.20.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Memory usage under different numbers of vertices. The random graphs were generated by fixing the number of edges at 500K and the dimension of input feature vectors at 32.\relax }}{23}{figure.caption.38}\protected@file@percent }
\newlabel{fig:exp_memory_expansion_ratio_input_graph_number_of_vertices_fixed_edge}{{21}{23}{Memory usage under different numbers of vertices. The random graphs were generated by fixing the number of edges at 500K and the dimension of input feature vectors at 32.\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Peak memory usage}}}{23}{subfigure.21.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Memory expansion ratio}}}{23}{subfigure.21.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Summary of Memory Usage}{23}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Effects of Sampling Techniques on Performance}{23}{subsection.4.4}\protected@file@percent }
\newlabel{sec:effects_of_sampling_techniques_on_performance}{{4.4}{23}{Effects of Sampling Techniques on Performance}{subsection.4.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Sizes of sampled subgraphs under different relative batch sizes. The batch size was relative to the full graph. Each batch size was sampled 50 times and the average values were reported. The error bar indicates the standard deviation.\relax }}{24}{figure.caption.40}\protected@file@percent }
\newlabel{fig:exp_sampling_minibatch_graph_info}{{22}{24}{Sizes of sampled subgraphs under different relative batch sizes. The batch size was relative to the full graph. Each batch size was sampled 50 times and the average values were reported. The error bar indicates the standard deviation.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Neighbor sampler}}}{24}{subfigure.22.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Cluster sampler}}}{24}{subfigure.22.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Vertex degree distribution of the sampled subgraph (relative batch size: 6\%) and the original graph. Dataset:\texttt  {amp}.\relax }}{24}{figure.caption.41}\protected@file@percent }
\newlabel{fig:exp_sampling_minibatch_degrees_distribution}{{23}{24}{Vertex degree distribution of the sampled subgraph (relative batch size: 6\%) and the original graph. Dataset:\texttt {amp}.\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Training time per batch breakdown. FULL means that the full graph participates in the training.\relax }}{25}{figure.caption.42}\protected@file@percent }
\newlabel{fig:exp_sampling_batch_train_time}{{24}{25}{Training time per batch breakdown. FULL means that the full graph participates in the training.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Neighbor sampler on \texttt {amc}}}}{25}{subfigure.24.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Neighbor sampler on \texttt {fli}}}}{25}{subfigure.24.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Cluster sampler on \texttt {amc}}}}{25}{subfigure.24.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Cluster sampler on \texttt {fli}}}}{25}{subfigure.24.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Peak memory usage under different batch sizes. FULL means that the full graph participated in the training.\relax }}{26}{figure.caption.43}\protected@file@percent }
\newlabel{fig:exp_sampling_memory_usage}{{25}{26}{Peak memory usage under different batch sizes. FULL means that the full graph participated in the training.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Neighbor sampler on \texttt {amc}}}}{26}{subfigure.25.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Neighbor sampler on \texttt {fli}}}}{26}{subfigure.25.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Cluster sampler on \texttt {amc}}}}{26}{subfigure.25.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Cluster sampler on \texttt {fli}}}}{26}{subfigure.25.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Training time per epoch on small random graphs. For each number of vertices, we generated 50 random graphs with the average degree of 4.0 and reported the average training time per batch (without the evaluation phase). The error bar indicates the standard deviation.\relax }}{26}{figure.caption.44}\protected@file@percent }
\newlabel{fig:exp_small_graph_train_time}{{26}{26}{Training time per epoch on small random graphs. For each number of vertices, we generated 50 random graphs with the average degree of 4.0 and reported the average training time per batch (without the evaluation phase). The error bar indicates the standard deviation.\relax }{figure.caption.44}{}}
\@writefile{toc}{\contentsline {paragraph}{Summary of Sampling Techniques}{26}{section*.45}\protected@file@percent }
\citation{DGL,PyG,ma2019_neugraph}
\citation{yan2020_characterizing_gcn,zhang2020_analysis_neugraph}
\citation{ma2019_neugraph}
\citation{chen2016_training_deep}
\citation{zhou2018_gnn_review}
\citation{zhang2018_gnn_survey}
\citation{comprehensive-survey-wu-2020}
\citation{shchur2018_pitfall_of_gnn}
\citation{dwivedi2020_benchmark_of_gnn}
\citation{hu2020_open_graph_benchmark}
\@writefile{toc}{\contentsline {section}{\numberline {5}Insights}{27}{section.5}\protected@file@percent }
\newlabel{sec:insights}{{5}{27}{Insights}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Work}{27}{section.6}\protected@file@percent }
\newlabel{sec:related_work}{{6}{27}{Related Work}{section.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Survey of GNNs}{27}{section*.46}\protected@file@percent }
\citation{yan2020_characterizing_gcn}
\citation{zhang2020_analysis_neugraph}
\citation{ma2019_neugraph}
\citation{yan2020_characterizing_gcn,zhang2020_analysis_neugraph}
\citation{PyG}
\citation{DGL}
\citation{PyG}
\citation{DGL}
\citation{ma2019_neugraph}
\citation{zhu2019_aligraph}
\citation{PGL}
\citation{*}
\bibdata{gnnref.bib}
\bibcite{kipf2017_gcn}{{1}{}{{}}{{}}}
\@writefile{toc}{\contentsline {paragraph}{Evaluation of GNNs}{28}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Libraries/Systems of GNNs}{28}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{28}{section.7}\protected@file@percent }
\newlabel{sec:conclusion}{{7}{28}{Conclusion}{section.7}{}}
\bibcite{defferrad2016_chebnet}{{2}{}{{}}{{}}}
\bibcite{li2018_agcn}{{3}{}{{}}{{}}}
\bibcite{li2015_ggnn}{{4}{}{{}}{{}}}
\bibcite{hamilton2017_graphsage}{{5}{}{{}}{{}}}
\bibcite{huang2018_gat}{{6}{}{{}}{{}}}
\bibcite{zhang2018_gaan}{{7}{}{{}}{{}}}
\bibcite{zhou2018_gnn_review}{{8}{}{{}}{{}}}
\bibcite{zhang2018_gnn_survey}{{9}{}{{}}{{}}}
\bibcite{comprehensive-survey-wu-2020}{{10}{}{{}}{{}}}
\bibcite{qi2018_learning_humanobject}{{11}{}{{}}{{}}}
\bibcite{wang2020_hierarchical_human_parsing}{{12}{}{{}}{{}}}
\bibcite{wang2019_zeroshot_video}{{13}{}{{}}{{}}}
\bibcite{PyG}{{14}{}{{}}{{}}}
\bibcite{DGL}{{15}{}{{}}{{}}}
\bibcite{ma2019_neugraph}{{16}{}{{}}{{}}}
\bibcite{zhu2019_aligraph}{{17}{}{{}}{{}}}
\bibcite{PGL}{{18}{}{{}}{{}}}
\bibcite{yan2020_characterizing_gcn}{{19}{}{{}}{{}}}
\bibcite{zhang2020_analysis_neugraph}{{20}{}{{}}{{}}}
\bibcite{bryan2014_deepwalk}{{21}{}{{}}{{}}}
\bibcite{aditya2016_node2vec}{{22}{}{{}}{{}}}
\bibcite{gilmer_messgae_passing}{{23}{}{{}}{{}}}
\bibcite{duvenaud2015_neural_fps}{{24}{}{{}}{{}}}
\bibcite{han2018_sse}{{25}{}{{}}{{}}}
\bibcite{chiang2019_cluster_gcn}{{26}{}{{}}{{}}}
\bibcite{ying2018_pinsage}{{27}{}{{}}{{}}}
\bibcite{chen2018_fastgcn}{{28}{}{{}}{{}}}
\bibcite{chen2018_sgcn}{{29}{}{{}}{{}}}
\bibcite{huang2018_adap}{{30}{}{{}}{{}}}
\bibcite{zeng2018_aesg}{{31}{}{{}}{{}}}
\bibcite{zeng2020_graphsaint}{{32}{}{{}}{{}}}
\bibcite{yang2016_revisiting_semisupervised}{{33}{}{{}}{{}}}
\bibcite{shchur2018_pitfall_of_gnn}{{34}{}{{}}{{}}}
\bibcite{rmat-generator}{{35}{}{{}}{{}}}
\bibcite{yang2012_defining}{{36}{}{{}}{{}}}
\bibcite{chen2016_training_deep}{{37}{}{{}}{{}}}
\bibcite{dwivedi2020_benchmark_of_gnn}{{38}{}{{}}{{}}}
\bibcite{hu2020_open_graph_benchmark}{{39}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
