%! TEX root = ms.tex
%! TEX program = pdflatex
\documentclass{elsarticle}
\usepackage{geometry}
\geometry{left=3cm, right=3cm, top=2.5cm, bottom=2.5cm} 
\usepackage{lineno,hyperref}
\usepackage{amsmath, amsthm}
\usepackage{relsize}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{lscape}
\usepackage{xcolor}
\usepackage{subfig}
\usepackage{float}
\usepackage{times}
\modulolinenumbers[5]
\bibliographystyle{elsarticle-num}


\begin{document}

\begin{frontmatter}

	\title{Empirical Analysis of Performance Bottlenecks in Graph Neural Network Training with GPUs}
    \author{Zhaokang Wang, Yunpan Wang, Chunfeng Yuan, Rong Gu$^*$ \corref{correspondingauthor1}, Yihua Huang$^*$ \corref{correspondingauthor2}}
    \cortext[correspondingauthor]{Corresponding authors with equal contribution}
    \ead{\{wangzhaokang, wangyp\}@smail.nju.edu.cn, \{cfyuan, gurong, yhuang\}@nju.edu.cn}
	\address{State Key Laboratory for Novel Software Technology, \\Department of Computer Science and Technology, Nanjing University, \\Nanjing 210023, China}

	\begin{abstract}
		The graph neural network (GNN) has become a popular research area for its state-of-the-art performance in many graph analysis tasks. 
		Recently, various graph neural network libraries have emerged.They make the development of GNNs convenient, but their performance on large datasets is not satisfying. 
		In this work, we analyze the performance bottleneck in training GNN with GPUs empirically. 
		A GNN layer can be decomposed into two parts: the vertex and the edge calculation parts. 
		According to their computational complexity, we select four representative GNNs (GCN, GGNN, GAT, GaAN) for evaluation. 
		We breakdown their training time and memory usage, evaluate the effects of hyper-parameters, and assess the efficiency of the sampling techniques.
		The experimental evaluation indicates that the edge-related calculation is the performance bottleneck for most GNNs, dominating the training time and memory usage.
		Future optimization can focus on it. The sampling techniques are essential for training big graphs on GPUs, but their current implementations still have room for improvement.
	\end{abstract}

	\begin{keyword}
		graph neural network, performance bottleneck analysis, empirical evaluation, machine learning system, GPU
	\end{keyword}

\end{frontmatter}

\linenumbers

\input{sec1-introduction.tex}
\input{sec2-review-of-graph-neural-network.tex}
\input{sec3-experiment-design.tex}
\input{sec4-experiments.tex}
\input{sec5-insights.tex}
\input{sec6-related-work.tex}
\input{sec7-conclusion.tex}

\nocite{*}% Show all bib entries - both cited and uncited; comment this line to view only cited bib entries;
\bibliography{gnnref.bib}

\end{document}
